setGPU: Setting GPU to: 0
tensorflow version:  2.3.1
[DataReader] read_events_from_dir(): reading 1000000 events from /eos/user/k/kiwoznia/data/VAE_data/events/qcd_sqrtshatTeV_13TeV_PU40_NEW_EXT_sideband_parts

num files read in dir  /eos/user/k/kiwoznia/data/VAE_data/events/qcd_sqrtshatTeV_13TeV_PU40_NEW_EXT_sideband_parts :  2
[DataReader] read_events_from_dir(): reading 1000000 events from /eos/user/k/kiwoznia/data/VAE_data/events/qcd_sqrtshatTeV_13TeV_PU40_NEW_sideband_parts

num files read in dir  /eos/user/k/kiwoznia/data/VAE_data/events/qcd_sqrtshatTeV_13TeV_PU40_NEW_sideband_parts :  2
computed mean [-2.5694157e-04  3.9219353e-07  3.1619492e+00] and std-dev [ 0.23610476  0.23951028 14.948415  ]
Model: "encoder"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
encoder_input (InputLayer)      [(None, 100, 3)]     0                                            
__________________________________________________________________________________________________
Std_Normalize (StdNormalization (None, 100, 3)       0           encoder_input[0][0]              
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 100, 3, 1)    0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 98, 1, 6)     60          lambda[0][0]                     
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 98, 6)        0           conv2d[0][0]                     
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 96, 10)       190         lambda_1[0][0]                   
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 94, 14)       434         conv1d[0][0]                     
__________________________________________________________________________________________________
average_pooling1d (AveragePooli (None, 47, 14)       0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
flatten (Flatten)               (None, 658)          0           average_pooling1d[0][0]          
__________________________________________________________________________________________________
dense (Dense)                   (None, 204)          134436      flatten[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 48)           9840        dense[0][0]                      
__________________________________________________________________________________________________
z_mean (Dense)                  (None, 12)           588         dense_1[0][0]                    
__________________________________________________________________________________________________
z_log_var (Dense)               (None, 12)           588         dense_1[0][0]                    
__________________________________________________________________________________________________
sampling (Sampling)             (None, 12)           0           z_mean[0][0]                     
                                                                 z_log_var[0][0]                  
==================================================================================================
Total params: 146,136
Trainable params: 146,136
Non-trainable params: 0
__________________________________________________________________________________________________
Model: "decoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
z_sampling (InputLayer)      [(None, 12)]              0         
_________________________________________________________________
dense_2 (Dense)              (None, 48)                624       
_________________________________________________________________
dense_3 (Dense)              (None, 204)               9996      
_________________________________________________________________
dense_4 (Dense)              (None, 658)               134890    
_________________________________________________________________
reshape (Reshape)            (None, 47, 14)            0         
_________________________________________________________________
up_sampling1d (UpSampling1D) (None, 94, 14)            0         
_________________________________________________________________
conv1d_transpose (Conv1DTran (None, 96, 10)            430       
_________________________________________________________________
conv1d_transpose_1 (Conv1DTr (None, 98, 6)             186       
_________________________________________________________________
lambda_6 (Lambda)            (None, 98, 1, 6)          0         
_________________________________________________________________
conv_2d_transpose (Conv2DTra (None, 100, 3, 1)         55        
_________________________________________________________________
lambda_7 (Lambda)            (None, 100, 3)            0         
_________________________________________________________________
Un_Normalize (StdUnnormaliza (None, 100, 3)            0         
=================================================================
Total params: 146,181
Trainable params: 146,181
Non-trainable params: 0
_________________________________________________________________
Model: "vae"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
model_input (InputLayer)     [(None, 100, 3)]          0         
_________________________________________________________________
encoder (Functional)         [(None, 12), (None, 12),  146136    
_________________________________________________________________
decoder (Functional)         (None, 100, 3)            146181    
=================================================================
Total params: 292,317
Trainable params: 292,317
Non-trainable params: 0
_________________________________________________________________

### [17.1 14:46:7] Start of epoch 0
Step 0: mean reco loss 224.3651, KL loss 3.2111 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.4384, KL loss 5.3879 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.3583, KL loss 4.8047 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.3196, KL loss 4.4137 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.2696, KL loss 4.2763 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.2560, KL loss 4.2449 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.2365, KL loss 4.0717 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1974, KL loss 4.0994 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1866, KL loss 3.8702 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1913, KL loss 3.8984 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 0 - 8500.03 sec]: train loss reco 0.494 kl 4.411, val loss reco 0.197 kl 3.830 (mean / batch) ###

### [17.1 17:7:47] Start of epoch 1
Step 0: mean reco loss 0.1934, KL loss 3.8262 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.2035, KL loss 3.7409 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.2156, KL loss 3.6507 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1696, KL loss 3.6024 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1618, KL loss 3.7074 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1710, KL loss 3.5052 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1555, KL loss 3.4254 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1645, KL loss 3.4235 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1923, KL loss 3.3858 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1548, KL loss 3.3268 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 1 - 8137.25 sec]: train loss reco 0.174 kl 3.524, val loss reco 0.156 kl 3.304 (mean / batch) ###

### [17.1 19:23:24] Start of epoch 2
Step 0: mean reco loss 0.1502, KL loss 3.2869 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1648, KL loss 3.2709 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1919, KL loss 3.3004 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1574, KL loss 3.2320 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1643, KL loss 3.2228 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1636, KL loss 3.1996 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1617, KL loss 3.1986 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1521, KL loss 3.1410 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1604, KL loss 3.2055 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1547, KL loss 3.1463 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 2 - 8014.78 sec]: train loss reco 0.162 kl 3.205, val loss reco 0.154 kl 3.126 (mean / batch) ###

### [17.1 21:36:59] Start of epoch 3
Step 0: mean reco loss 0.1527, KL loss 3.1608 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1500, KL loss 3.1931 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1474, KL loss 3.0635 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1460, KL loss 3.0501 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1503, KL loss 3.1527 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1594, KL loss 3.0646 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1471, KL loss 3.0942 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1491, KL loss 3.0891 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1540, KL loss 3.0504 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1512, KL loss 3.0223 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 3 - 9008.11 sec]: train loss reco 0.158 kl 3.094, val loss reco 0.159 kl 3.056 (mean / batch) ###

### [18.1 0:7:7] Start of epoch 4
Step 0: mean reco loss 0.1548, KL loss 3.0440 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1508, KL loss 3.0717 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1476, KL loss 3.0310 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1790, KL loss 3.0554 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1456, KL loss 3.0530 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1577, KL loss 2.9665 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1774, KL loss 3.0276 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1511, KL loss 3.0998 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1476, KL loss 2.9687 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1463, KL loss 2.9642 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 4 - 8772.92 sec]: train loss reco 0.153 kl 3.029, val loss reco 0.147 kl 3.007 (mean / batch) ###
saving best so far model with valid loss 0.147 and kl loss 3.007
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [18.1 2:33:21] Start of epoch 5
Step 0: mean reco loss 0.1480, KL loss 2.9518 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1452, KL loss 3.0323 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1526, KL loss 2.9945 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1444, KL loss 3.0201 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1471, KL loss 3.0468 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1525, KL loss 3.0812 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1409, KL loss 2.9770 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1535, KL loss 3.0070 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1440, KL loss 3.0479 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1483, KL loss 3.0479 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 5 - 8860.75 sec]: train loss reco 0.150 kl 3.034, val loss reco 0.142 kl 3.075 (mean / batch) ###
saving best so far model with valid loss 0.142 and kl loss 3.075
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [18.1 5:1:3] Start of epoch 6
Step 0: mean reco loss 0.1432, KL loss 3.0307 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1375, KL loss 3.0336 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1529, KL loss 3.0462 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1612, KL loss 3.0681 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1476, KL loss 3.0977 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1383, KL loss 3.0093 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1675, KL loss 3.0819 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1447, KL loss 3.0162 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1490, KL loss 3.0218 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1442, KL loss 3.1381 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 6 - 8189.47 sec]: train loss reco 0.147 kl 3.062, val loss reco 0.150 kl 3.076 (mean / batch) ###

### [18.1 7:17:33] Start of epoch 7
Step 0: mean reco loss 0.1512, KL loss 3.1396 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1441, KL loss 3.0885 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1419, KL loss 3.0991 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1402, KL loss 3.0781 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1449, KL loss 3.0132 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1498, KL loss 3.0572 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1437, KL loss 3.0680 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1415, KL loss 3.0655 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1424, KL loss 3.0320 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1455, KL loss 3.0346 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 7 - 8181.16 sec]: train loss reco 0.146 kl 3.053, val loss reco 0.140 kl 3.063 (mean / batch) ###
saving best so far model with valid loss 0.140 and kl loss 3.063
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [18.1 9:33:55] Start of epoch 8
Step 0: mean reco loss 0.1377, KL loss 3.0608 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1389, KL loss 3.1175 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1521, KL loss 3.0210 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1449, KL loss 2.9775 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1440, KL loss 3.0385 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1537, KL loss 3.0333 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1627, KL loss 3.0213 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1420, KL loss 3.0473 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1402, KL loss 3.0237 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1442, KL loss 2.9638 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 8 - 7879.45 sec]: train loss reco 0.144 kl 3.033, val loss reco 0.141 kl 3.023 (mean / batch) ###

### [18.1 11:45:14] Start of epoch 9
Step 0: mean reco loss 0.1433, KL loss 3.0734 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1380, KL loss 3.0180 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1407, KL loss 3.0036 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1419, KL loss 3.0102 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1394, KL loss 3.0245 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1430, KL loss 3.0429 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1434, KL loss 3.0219 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1387, KL loss 3.0069 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1448, KL loss 2.9991 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1576, KL loss 3.0349 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 9 - 8590.85 sec]: train loss reco 0.146 kl 3.026, val loss reco 0.140 kl 3.086 (mean / batch) ###
saving best so far model with valid loss 0.140 and kl loss 3.086
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [18.1 14:8:27] Start of epoch 10
Step 0: mean reco loss 0.1406, KL loss 3.1058 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1383, KL loss 3.0679 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1445, KL loss 3.0499 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1386, KL loss 3.0242 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1415, KL loss 3.0437 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1397, KL loss 3.0372 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1440, KL loss 2.9955 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1383, KL loss 3.0095 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1403, KL loss 3.0633 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1539, KL loss 2.9816 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 10 - 8476.39 sec]: train loss reco 0.144 kl 3.029, val loss reco 0.142 kl 3.030 (mean / batch) ###
------- Early stopping for last 4 validation losses [0.17087813, 0.17078906, 0.17105132, 0.17215075]-------
decreasing learning rate from 1.000e-03 to 3.000e-04

### [18.1 16:29:44] Start of epoch 11
Step 0: mean reco loss 0.1407, KL loss 3.0011 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1385, KL loss 3.0326 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1392, KL loss 3.0365 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1367, KL loss 2.9577 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1383, KL loss 2.9726 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1358, KL loss 2.9699 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1345, KL loss 3.0138 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1339, KL loss 3.0181 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1384, KL loss 3.0057 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1330, KL loss 3.0242 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 11 - 8974.99 sec]: train loss reco 0.136 kl 3.012, val loss reco 0.136 kl 2.991 (mean / batch) ###
saving best so far model with valid loss 0.136 and kl loss 2.991
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [18.1 18:59:24] Start of epoch 12
Step 0: mean reco loss 0.1364, KL loss 3.0302 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1370, KL loss 3.0656 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1369, KL loss 3.0193 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1345, KL loss 3.0152 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1344, KL loss 2.9372 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1381, KL loss 2.9469 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1305, KL loss 2.9673 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1403, KL loss 3.0207 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1361, KL loss 3.0056 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1351, KL loss 2.9593 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 12 - 8545.84 sec]: train loss reco 0.136 kl 2.999, val loss reco 0.135 kl 3.005 (mean / batch) ###
saving best so far model with valid loss 0.135 and kl loss 3.005
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [18.1 21:21:51] Start of epoch 13
Step 0: mean reco loss 0.1359, KL loss 2.9999 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1341, KL loss 2.9787 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1378, KL loss 2.9725 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1345, KL loss 3.0135 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1360, KL loss 3.0121 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1344, KL loss 2.9726 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1371, KL loss 3.0268 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1330, KL loss 3.0089 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1345, KL loss 3.0042 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1343, KL loss 2.9793 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 13 - 8897.26 sec]: train loss reco 0.135 kl 2.996, val loss reco 0.135 kl 3.006 (mean / batch) ###
saving best so far model with valid loss 0.135 and kl loss 3.006
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [18.1 23:50:10] Start of epoch 14
Step 0: mean reco loss 0.1367, KL loss 3.0160 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1309, KL loss 3.0043 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1363, KL loss 2.9813 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1349, KL loss 2.9669 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1342, KL loss 2.9747 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1318, KL loss 2.9780 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1334, KL loss 3.0057 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1344, KL loss 2.9968 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1348, KL loss 2.9575 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1310, KL loss 3.0167 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 14 - 8490.61 sec]: train loss reco 0.135 kl 2.994, val loss reco 0.135 kl 2.975 (mean / batch) ###
------- Early stopping for last 4 validation losses [0.16573203, 0.16515413, 0.16486008, 0.16445275]-------
decreasing learning rate from 3.000e-04 to 9.000e-05
saving best so far model with valid loss 0.135 and kl loss 2.975
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [19.1 2:11:41] Start of epoch 15
Step 0: mean reco loss 0.1336, KL loss 2.9475 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1363, KL loss 3.0490 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1351, KL loss 2.9661 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1298, KL loss 2.9719 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1330, KL loss 3.0398 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1304, KL loss 2.9745 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1324, KL loss 2.9756 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1327, KL loss 2.9772 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1370, KL loss 3.0005 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1324, KL loss 2.9860 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 15 - 9175.60 sec]: train loss reco 0.134 kl 2.995, val loss reco 0.133 kl 3.000 (mean / batch) ###
------- Early stopping for last 4 validation losses [0.16515413, 0.16486008, 0.16445275, 0.16332114]-------
decreasing learning rate from 9.000e-05 to 2.700e-05
saving best so far model with valid loss 0.133 and kl loss 3.000
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [19.1 4:44:38] Start of epoch 16
Step 0: mean reco loss 0.1334, KL loss 3.0643 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1321, KL loss 3.0154 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1301, KL loss 2.9756 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1325, KL loss 2.9961 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1318, KL loss 3.0225 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1348, KL loss 3.0255 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1360, KL loss 3.0022 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1343, KL loss 2.9754 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1318, KL loss 2.9824 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1340, KL loss 3.0216 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 16 - 8521.55 sec]: train loss reco 0.133 kl 2.996, val loss reco 0.133 kl 2.990 (mean / batch) ###
------- Early stopping for last 4 validation losses [0.16486008, 0.16445275, 0.16332114, 0.16286197]-------
decreasing learning rate from 2.700e-05 to 8.100e-06
saving best so far model with valid loss 0.133 and kl loss 2.990
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [19.1 7:6:41] Start of epoch 17
Step 0: mean reco loss 0.1308, KL loss 3.0513 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1346, KL loss 2.9862 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1338, KL loss 3.0072 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1335, KL loss 2.9910 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1332, KL loss 2.9706 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1322, KL loss 2.9633 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1334, KL loss 2.9560 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1291, KL loss 2.9741 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1302, KL loss 2.9860 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1337, KL loss 3.0289 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 17 - 8252.20 sec]: train loss reco 0.133 kl 2.992, val loss reco 0.133 kl 2.991 (mean / batch) ###
------- Early stopping for last 4 validation losses [0.16445275, 0.16332114, 0.16286197, 0.16266002]-------
decreasing learning rate from 8.100e-06 to 2.430e-06
saving best so far model with valid loss 0.133 and kl loss 2.991
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [19.1 9:24:14] Start of epoch 18
Step 0: mean reco loss 0.1344, KL loss 2.9840 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1327, KL loss 3.0099 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1316, KL loss 3.0363 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1323, KL loss 2.9996 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1309, KL loss 3.0234 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1319, KL loss 2.9606 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1314, KL loss 2.9408 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1319, KL loss 3.0104 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1330, KL loss 3.0348 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1334, KL loss 2.9708 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 18 - 8077.06 sec]: train loss reco 0.133 kl 2.992, val loss reco 0.133 kl 2.993 (mean / batch) ###
------- Early stopping for last 4 validation losses [0.16332114, 0.16286197, 0.16266002, 0.16260518]-------
decreasing learning rate from 2.430e-06 to 7.290e-07
saving best so far model with valid loss 0.133 and kl loss 2.993
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [19.1 11:38:54] Start of epoch 19
Step 0: mean reco loss 0.1333, KL loss 3.0349 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1352, KL loss 3.0149 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1331, KL loss 2.9750 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1351, KL loss 2.9979 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1353, KL loss 3.0289 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1317, KL loss 3.0315 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1349, KL loss 2.9636 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1346, KL loss 3.0413 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1335, KL loss 2.9938 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1327, KL loss 2.9605 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 19 - 8248.67 sec]: train loss reco 0.133 kl 2.993, val loss reco 0.133 kl 2.992 (mean / batch) ###
------- Early stopping for last 4 validation losses [0.16286197, 0.16266002, 0.16260518, 0.16258457]-------
decreasing learning rate from 7.290e-07 to 2.187e-07
saving best so far model with valid loss 0.133 and kl loss 2.992
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [19.1 13:56:23] Start of epoch 20
Step 0: mean reco loss 0.1318, KL loss 3.0097 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1305, KL loss 2.9731 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1313, KL loss 2.9162 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1367, KL loss 3.0175 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1341, KL loss 2.9995 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1351, KL loss 2.9564 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1309, KL loss 2.9542 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1347, KL loss 2.9745 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1339, KL loss 3.0111 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1332, KL loss 2.9610 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 20 - 8014.04 sec]: train loss reco 0.133 kl 2.993, val loss reco 0.133 kl 2.992 (mean / batch) ###
------- Early stopping for last 4 validation losses [0.16266002, 0.16260518, 0.16258457, 0.16258214]-------
decreasing learning rate from 2.187e-07 to 6.561e-08
saving best so far model with valid loss 0.133 and kl loss 2.992
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [19.1 16:9:58] Start of epoch 21
Step 0: mean reco loss 0.1350, KL loss 2.9939 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1351, KL loss 2.9826 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1324, KL loss 2.9469 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1334, KL loss 3.0583 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1330, KL loss 2.9884 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1349, KL loss 3.0277 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1324, KL loss 2.9830 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1320, KL loss 3.0052 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1313, KL loss 3.0136 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1321, KL loss 2.9733 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 21 - 8633.79 sec]: train loss reco 0.133 kl 2.993, val loss reco 0.133 kl 2.992 (mean / batch) ###
------- Early stopping for last 4 validation losses [0.16260518, 0.16258457, 0.16258214, 0.16257754]-------
decreasing learning rate from 6.561e-08 to 1.968e-08
saving best so far model with valid loss 0.133 and kl loss 2.992
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [19.1 18:33:59] Start of epoch 22
Step 0: mean reco loss 0.1322, KL loss 3.0579 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1296, KL loss 2.9689 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1316, KL loss 2.9489 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1319, KL loss 2.9665 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1320, KL loss 2.9864 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1313, KL loss 2.9839 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1324, KL loss 3.0411 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1324, KL loss 3.0262 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1310, KL loss 2.9919 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1317, KL loss 2.9512 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 22 - 8126.17 sec]: train loss reco 0.133 kl 2.993, val loss reco 0.133 kl 2.992 (mean / batch) ###
------- Early stopping for last 4 validation losses [0.16258457, 0.16258214, 0.16257754, 0.16257459]-------
decreasing learning rate from 1.968e-08 to 5.905e-09
saving best so far model with valid loss 0.133 and kl loss 2.992
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109/best_so_far

### [19.1 20:49:26] Start of epoch 23
Step 0: mean reco loss 0.1321, KL loss 2.9789 (in one batch)
Seen so far: 256 samples
Step 4000: mean reco loss 0.1359, KL loss 2.9914 (in one batch)
Seen so far: 1024256 samples
Step 8000: mean reco loss 0.1335, KL loss 3.0253 (in one batch)
Seen so far: 2048256 samples
Step 12000: mean reco loss 0.1300, KL loss 2.9734 (in one batch)
Seen so far: 3072256 samples
Step 16000: mean reco loss 0.1328, KL loss 3.0323 (in one batch)
Seen so far: 4096256 samples
Step 20000: mean reco loss 0.1324, KL loss 3.0260 (in one batch)
Seen so far: 5120256 samples
Step 24000: mean reco loss 0.1318, KL loss 2.9887 (in one batch)
Seen so far: 6144256 samples
Step 28000: mean reco loss 0.1304, KL loss 2.9911 (in one batch)
Seen so far: 7168256 samples
Step 32000: mean reco loss 0.1317, KL loss 2.9944 (in one batch)
Seen so far: 8192256 samples
Step 36000: mean reco loss 0.1329, KL loss 2.9782 (in one batch)
Seen so far: 9216256 samples
[DataGenerator]: __call__() yielded 10000000 samples
### [Epoch 23 - 8304.27 sec]: train loss reco 0.133 kl 2.993, val loss reco 0.133 kl 2.992 (mean / batch) ###
------- Early stopping for last 4 validation losses [0.16258214, 0.16257754, 0.16257459, 0.16257516]-------
!!! stopping training !!!
saving model to /eos/home-k/kiwoznia/data/VAE_models/run_109

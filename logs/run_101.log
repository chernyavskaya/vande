setGPU: Setting GPU to: 0
2020-07-24 14:52:29.189065: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/afs/cern.ch/user/k/kiwoznia/.local/lib:/afs/cern.ch/user/k/kiwoznia/software/cuda/lib64:/usr/local/cuda-10.0/lib64
2020-07-24 14:52:29.189563: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/afs/cern.ch/user/k/kiwoznia/.local/lib:/afs/cern.ch/user/k/kiwoznia/software/cuda/lib64:/usr/local/cuda-10.0/lib64
2020-07-24 14:52:29.189587: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Python:  3.6.0 |Anaconda custom (64-bit)| (default, Dec 23 2016, 12:22:00) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
Tensorflow:  2.1.0
2020-07-24 14:53:56.476376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-24 14:53:59.710804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-07-24 14:53:59.711865: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/afs/cern.ch/user/k/kiwoznia/.local/lib:/afs/cern.ch/user/k/kiwoznia/software/cuda/lib64:/usr/local/cuda-10.0/lib64
2020-07-24 14:53:59.794914: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-24 14:53:59.795771: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/afs/cern.ch/user/k/kiwoznia/.local/lib:/afs/cern.ch/user/k/kiwoznia/software/cuda/lib64:/usr/local/cuda-10.0/lib64
2020-07-24 14:53:59.796288: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/afs/cern.ch/user/k/kiwoznia/.local/lib:/afs/cern.ch/user/k/kiwoznia/software/cuda/lib64:/usr/local/cuda-10.0/lib64
2020-07-24 14:53:59.796799: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/afs/cern.ch/user/k/kiwoznia/.local/lib:/afs/cern.ch/user/k/kiwoznia/software/cuda/lib64:/usr/local/cuda-10.0/lib64
2020-07-24 14:53:59.797507: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/afs/cern.ch/user/k/kiwoznia/.local/lib:/afs/cern.ch/user/k/kiwoznia/software/cuda/lib64:/usr/local/cuda-10.0/lib64
2020-07-24 14:54:00.773785: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-24 14:54:00.773870: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-07-24 14:54:00.774891: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-07-24 14:54:00.800486: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200040000 Hz
2020-07-24 14:54:00.808110: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1362ebc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-24 14:54:00.808166: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-24 14:54:01.003143: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14536260 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-24 14:54:01.003206: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-07-24 14:54:01.003417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-24 14:54:01.003437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      
Model: "encoder"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
encoder_input (InputLayer)      [(None, 100, 3)]     0                                            
__________________________________________________________________________________________________
normalization (Normalization)   (None, 100, 3)       7           encoder_input[0][0]              
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 100, 3, 1)    0           normalization[0][0]              
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 98, 1, 6)     60          lambda[0][0]                     
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 98, 6)        0           conv2d[0][0]                     
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 96, 10)       190         lambda_1[0][0]                   
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 94, 14)       434         conv1d[0][0]                     
__________________________________________________________________________________________________
average_pooling1d (AveragePooli (None, 47, 14)       0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
flatten (Flatten)               (None, 658)          0           average_pooling1d[0][0]          
__________________________________________________________________________________________________
dense (Dense)                   (None, 38)           25042       flatten[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 15)           585         dense[0][0]                      
__________________________________________________________________________________________________
z_mean (Dense)                  (None, 10)           160         dense_1[0][0]                    
__________________________________________________________________________________________________
z_log_var (Dense)               (None, 10)           160         dense_1[0][0]                    
__________________________________________________________________________________________________
sampling (Sampling)             (None, 10)           0           z_mean[0][0]                     
                                                                 z_log_var[0][0]                  
==================================================================================================
Total params: 26,638
Trainable params: 26,631
Non-trainable params: 7
__________________________________________________________________________________________________
Model: "decoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
z_sampling (InputLayer)      [(None, 10)]              0         
_________________________________________________________________
dense_2 (Dense)              (None, 15)                165       
_________________________________________________________________
dense_3 (Dense)              (None, 38)                608       
_________________________________________________________________
dense_4 (Dense)              (None, 658)               25662     
_________________________________________________________________
reshape (Reshape)            (None, 47, 14)            0         
_________________________________________________________________
up_sampling1d (UpSampling1D) (None, 94, 14)            0         
_________________________________________________________________
conv1d_transpose (Conv1DTran (None, 96, 10)            430       
_________________________________________________________________
conv1d_transpose_1 (Conv1DTr (None, 98, 6)             186       
_________________________________________________________________
lambda_6 (Lambda)            (None, 98, 1, 6)          0         
_________________________________________________________________
conv_2d_transpose (Conv2DTra (None, 100, 3, 1)         55        
_________________________________________________________________
decoder_output (Lambda)      (None, 100, 3)            0         
_________________________________________________________________
lambda_7 (Lambda)            (None, 100, 3)            0         
=================================================================
Total params: 27,106
Trainable params: 27,106
Non-trainable params: 0
_________________________________________________________________
Model: "vae"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
encoder_input (InputLayer)      [(None, 100, 3)]     0                                            
__________________________________________________________________________________________________
normalization (Normalization)   (None, 100, 3)       7           encoder_input[0][0]              
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 100, 3, 1)    0           normalization[0][0]              
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 98, 1, 6)     60          lambda[0][0]                     
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 98, 6)        0           conv2d[0][0]                     
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 96, 10)       190         lambda_1[0][0]                   
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 94, 14)       434         conv1d[0][0]                     
__________________________________________________________________________________________________
average_pooling1d (AveragePooli (None, 47, 14)       0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
flatten (Flatten)               (None, 658)          0           average_pooling1d[0][0]          
__________________________________________________________________________________________________
dense (Dense)                   (None, 38)           25042       flatten[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 15)           585         dense[0][0]                      
__________________________________________________________________________________________________
z_mean (Dense)                  (None, 10)           160         dense_1[0][0]                    
__________________________________________________________________________________________________
z_log_var (Dense)               (None, 10)           160         dense_1[0][0]                    
__________________________________________________________________________________________________
sampling (Sampling)             (None, 10)           0           z_mean[0][0]                     
                                                                 z_log_var[0][0]                  
__________________________________________________________________________________________________
decoder (Model)                 (None, 100, 3)       27106       sampling[0][0]                   
==================================================================================================
Total params: 53,744
Trainable params: 53,737
Non-trainable params: 7
__________________________________________________________________________________________________
Train on 2253555 samples, validate on 751185 samples
Epoch 1/100
2253555/2253555 - 503s - loss: 423.7042 - threeD_loss: 422.8271 - kl_loss: 87.6932 - val_loss: 102.6688 - val_threeD_loss: 101.7710 - val_kl_loss: 89.7751
Epoch 2/100
2253555/2253555 - 444s - loss: 85.6848 - threeD_loss: 84.8926 - kl_loss: 79.1876 - val_loss: 69.4364 - val_threeD_loss: 68.7114 - val_kl_loss: 72.4899
Epoch 3/100
2253555/2253555 - 582s - loss: 43.4948 - threeD_loss: 42.7690 - kl_loss: 72.5677 - val_loss: 37.2044 - val_threeD_loss: 36.5059 - val_kl_loss: 69.8440
Epoch 4/100
2253555/2253555 - 584s - loss: 38.4990 - threeD_loss: 37.8098 - kl_loss: 68.9189 - val_loss: 36.3555 - val_threeD_loss: 35.6786 - val_kl_loss: 67.6861
Epoch 5/100
2253555/2253555 - 553s - loss: 37.3451 - threeD_loss: 36.6729 - kl_loss: 67.2215 - val_loss: 35.5001 - val_threeD_loss: 34.8347 - val_kl_loss: 66.5404
Epoch 6/100
2253555/2253555 - 439s - loss: 36.5817 - threeD_loss: 35.9177 - kl_loss: 66.3990 - val_loss: 35.3167 - val_threeD_loss: 34.6561 - val_kl_loss: 66.0527
Epoch 7/100
2253555/2253555 - 415s - loss: 37.4545 - threeD_loss: 36.7984 - kl_loss: 65.6145 - val_loss: 37.7054 - val_threeD_loss: 37.0469 - val_kl_loss: 65.8551
Epoch 8/100
2253555/2253555 - 410s - loss: 35.8969 - threeD_loss: 35.2456 - kl_loss: 65.1172 - val_loss: 33.6117 - val_threeD_loss: 32.9640 - val_kl_loss: 64.7772
Epoch 9/100
2253555/2253555 - 434s - loss: 35.3972 - threeD_loss: 34.7510 - kl_loss: 64.6219 - val_loss: 34.1684 - val_threeD_loss: 33.5241 - val_kl_loss: 64.4244
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
2253555/2253555 - 421s - loss: 35.4365 - threeD_loss: 34.7942 - kl_loss: 64.2303 - val_loss: 34.2708 - val_threeD_loss: 33.6336 - val_kl_loss: 63.7161
Epoch 11/100
2253555/2253555 - 476s - loss: 32.3479 - threeD_loss: 31.7032 - kl_loss: 64.4708 - val_loss: 32.0901 - val_threeD_loss: 31.4435 - val_kl_loss: 64.6596
Epoch 12/100
2253555/2253555 - 500s - loss: 31.7717 - threeD_loss: 31.1240 - kl_loss: 64.7666 - val_loss: 31.3878 - val_threeD_loss: 30.7391 - val_kl_loss: 64.8745
Epoch 13/100
2253555/2253555 - 464s - loss: 30.5212 - threeD_loss: 29.8715 - kl_loss: 64.9642 - val_loss: 30.0903 - val_threeD_loss: 29.4377 - val_kl_loss: 65.2605
Epoch 14/100
2253555/2253555 - 460s - loss: 29.5262 - threeD_loss: 28.8746 - kl_loss: 65.1654 - val_loss: 29.4544 - val_threeD_loss: 28.8024 - val_kl_loss: 65.1949
Epoch 15/100
2253555/2253555 - 439s - loss: 29.1234 - threeD_loss: 28.4722 - kl_loss: 65.1170 - val_loss: 28.9824 - val_threeD_loss: 28.3297 - val_kl_loss: 65.2719
Epoch 16/100
2253555/2253555 - 462s - loss: 28.8732 - threeD_loss: 28.2225 - kl_loss: 65.0756 - val_loss: 28.8623 - val_threeD_loss: 28.2118 - val_kl_loss: 65.0401
Epoch 17/100
2253555/2253555 - 443s - loss: 28.6638 - threeD_loss: 28.0136 - kl_loss: 65.0143 - val_loss: 28.5973 - val_threeD_loss: 27.9481 - val_kl_loss: 64.9160
Epoch 18/100
2253555/2253555 - 443s - loss: 28.4506 - threeD_loss: 27.8012 - kl_loss: 64.9356 - val_loss: 28.1958 - val_threeD_loss: 27.5466 - val_kl_loss: 64.9157
Epoch 19/100
2253555/2253555 - 513s - loss: 27.9935 - threeD_loss: 27.3446 - kl_loss: 64.8829 - val_loss: 27.5493 - val_threeD_loss: 26.8981 - val_kl_loss: 65.1284
Epoch 20/100
2253555/2253555 - 603s - loss: 25.4385 - threeD_loss: 24.7803 - kl_loss: 65.8234 - val_loss: 24.4972 - val_threeD_loss: 23.8333 - val_kl_loss: 66.3895
Epoch 21/100
2253555/2253555 - 606s - loss: 23.8913 - threeD_loss: 23.2227 - kl_loss: 66.8574 - val_loss: 23.2526 - val_threeD_loss: 22.5776 - val_kl_loss: 67.4909
Epoch 22/100
2253555/2253555 - 616s - loss: 22.1223 - threeD_loss: 21.4448 - kl_loss: 67.7541 - val_loss: 21.7628 - val_threeD_loss: 21.0859 - val_kl_loss: 67.6821
Epoch 23/100
2253555/2253555 - 608s - loss: 21.6350 - threeD_loss: 20.9579 - kl_loss: 67.7095 - val_loss: 21.5362 - val_threeD_loss: 20.8593 - val_kl_loss: 67.6837
Epoch 24/100
2253555/2253555 - 609s - loss: 21.4988 - threeD_loss: 20.8232 - kl_loss: 67.5668 - val_loss: 21.3906 - val_threeD_loss: 20.7159 - val_kl_loss: 67.4796
Epoch 25/100
2253555/2253555 - 610s - loss: 21.4075 - threeD_loss: 20.7337 - kl_loss: 67.3835 - val_loss: 21.3352 - val_threeD_loss: 20.6619 - val_kl_loss: 67.3332
Epoch 26/100
2253555/2253555 - 608s - loss: 21.3284 - threeD_loss: 20.6561 - kl_loss: 67.2281 - val_loss: 21.3355 - val_threeD_loss: 20.6647 - val_kl_loss: 67.0851
Epoch 27/100
2253555/2253555 - 596s - loss: 21.2627 - threeD_loss: 20.5918 - kl_loss: 67.0953 - val_loss: 21.2193 - val_threeD_loss: 20.5489 - val_kl_loss: 67.0387
Epoch 28/100
2253555/2253555 - 595s - loss: 21.2212 - threeD_loss: 20.5515 - kl_loss: 66.9621 - val_loss: 21.1606 - val_threeD_loss: 20.4909 - val_kl_loss: 66.9749
Epoch 29/100
2253555/2253555 - 594s - loss: 21.2021 - threeD_loss: 20.5336 - kl_loss: 66.8522 - val_loss: 21.3404 - val_threeD_loss: 20.6726 - val_kl_loss: 66.7864
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
2253555/2253555 - 594s - loss: 21.1776 - threeD_loss: 20.5105 - kl_loss: 66.7085 - val_loss: 21.1628 - val_threeD_loss: 20.4962 - val_kl_loss: 66.6565
Epoch 31/100
2253555/2253555 - 595s - loss: 21.0182 - threeD_loss: 20.3509 - kl_loss: 66.7317 - val_loss: 21.0234 - val_threeD_loss: 20.3559 - val_kl_loss: 66.7533
Epoch 32/100
2253555/2253555 - 596s - loss: 21.0147 - threeD_loss: 20.3470 - kl_loss: 66.7661 - val_loss: 21.0320 - val_threeD_loss: 20.3643 - val_kl_loss: 66.7670
Epoch 33/100
2253555/2253555 - 591s - loss: 21.0117 - threeD_loss: 20.3440 - kl_loss: 66.7677 - val_loss: 21.0199 - val_threeD_loss: 20.3521 - val_kl_loss: 66.7796
Epoch 34/100
2253555/2253555 - 597s - loss: 21.0092 - threeD_loss: 20.3415 - kl_loss: 66.7675 - val_loss: 21.0215 - val_threeD_loss: 20.3538 - val_kl_loss: 66.7677
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
2253555/2253555 - 608s - loss: 21.0065 - threeD_loss: 20.3387 - kl_loss: 66.7729 - val_loss: 21.0263 - val_threeD_loss: 20.3587 - val_kl_loss: 66.7567
Epoch 36/100
2253555/2253555 - 603s - loss: 20.9852 - threeD_loss: 20.3175 - kl_loss: 66.7702 - val_loss: 20.9982 - val_threeD_loss: 20.3306 - val_kl_loss: 66.7642
Epoch 37/100
2253555/2253555 - 603s - loss: 20.9842 - threeD_loss: 20.3166 - kl_loss: 66.7730 - val_loss: 20.9958 - val_threeD_loss: 20.3281 - val_kl_loss: 66.7683
Epoch 38/100
2253555/2253555 - 605s - loss: 20.9841 - threeD_loss: 20.3164 - kl_loss: 66.7729 - val_loss: 20.9953 - val_threeD_loss: 20.3276 - val_kl_loss: 66.7754
Epoch 39/100
2253555/2253555 - 602s - loss: 20.9838 - threeD_loss: 20.3162 - kl_loss: 66.7743 - val_loss: 20.9958 - val_threeD_loss: 20.3281 - val_kl_loss: 66.7683
Epoch 40/100
2253555/2253555 - 604s - loss: 20.9835 - threeD_loss: 20.3158 - kl_loss: 66.7733 - val_loss: 20.9950 - val_threeD_loss: 20.3274 - val_kl_loss: 66.7668
Epoch 41/100
2253555/2253555 - 603s - loss: 20.9831 - threeD_loss: 20.3154 - kl_loss: 66.7757 - val_loss: 20.9954 - val_threeD_loss: 20.3276 - val_kl_loss: 66.7783
Epoch 42/100
2253555/2253555 - 604s - loss: 20.9829 - threeD_loss: 20.3151 - kl_loss: 66.7778 - val_loss: 20.9948 - val_threeD_loss: 20.3270 - val_kl_loss: 66.7795
Epoch 43/100
2253555/2253555 - 604s - loss: 20.9824 - threeD_loss: 20.3147 - kl_loss: 66.7782 - val_loss: 20.9952 - val_threeD_loss: 20.3274 - val_kl_loss: 66.7774
Epoch 44/100
2253555/2253555 - 603s - loss: 20.9820 - threeD_loss: 20.3143 - kl_loss: 66.7778 - val_loss: 20.9944 - val_threeD_loss: 20.3266 - val_kl_loss: 66.7773
Epoch 45/100
2253555/2253555 - 603s - loss: 20.9820 - threeD_loss: 20.3143 - kl_loss: 66.7767 - val_loss: 20.9937 - val_threeD_loss: 20.3261 - val_kl_loss: 66.7679
Epoch 46/100
2253555/2253555 - 606s - loss: 20.9816 - threeD_loss: 20.3139 - kl_loss: 66.7709 - val_loss: 20.9954 - val_threeD_loss: 20.3276 - val_kl_loss: 66.7780
Epoch 47/100

Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.
2253555/2253555 - 603s - loss: 20.9812 - threeD_loss: 20.3136 - kl_loss: 66.7726 - val_loss: 20.9942 - val_threeD_loss: 20.3264 - val_kl_loss: 66.7758
Epoch 48/100
2253555/2253555 - 612s - loss: 20.9789 - threeD_loss: 20.3112 - kl_loss: 66.7715 - val_loss: 20.9915 - val_threeD_loss: 20.3238 - val_kl_loss: 66.7722
Epoch 49/100
2253555/2253555 - 599s - loss: 20.9787 - threeD_loss: 20.3109 - kl_loss: 66.7721 - val_loss: 20.9915 - val_threeD_loss: 20.3238 - val_kl_loss: 66.7718
Epoch 50/100

Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.
2253555/2253555 - 603s - loss: 20.9785 - threeD_loss: 20.3109 - kl_loss: 66.7725 - val_loss: 20.9918 - val_threeD_loss: 20.3241 - val_kl_loss: 66.7720
Epoch 51/100
2253555/2253555 - 602s - loss: 20.9785 - threeD_loss: 20.3108 - kl_loss: 66.7729 - val_loss: 20.9914 - val_threeD_loss: 20.3237 - val_kl_loss: 66.7723
Epoch 52/100
2253555/2253555 - 605s - loss: 20.9782 - threeD_loss: 20.3105 - kl_loss: 66.7731 - val_loss: 20.9909 - val_threeD_loss: 20.3231 - val_kl_loss: 66.7725
Epoch 53/100
2253555/2253555 - 602s - loss: 20.9787 - threeD_loss: 20.3110 - kl_loss: 66.7729 - val_loss: 20.9911 - val_threeD_loss: 20.3234 - val_kl_loss: 66.7724
Epoch 54/100

Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.
2253555/2253555 - 604s - loss: 20.9784 - threeD_loss: 20.3107 - kl_loss: 66.7733 - val_loss: 20.9918 - val_threeD_loss: 20.3241 - val_kl_loss: 66.7721
Epoch 55/100
2253555/2253555 - 602s - loss: 20.9787 - threeD_loss: 20.3109 - kl_loss: 66.7728 - val_loss: 20.9908 - val_threeD_loss: 20.3231 - val_kl_loss: 66.7721
Epoch 56/100

Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.
2253555/2253555 - 603s - loss: 20.9785 - threeD_loss: 20.3107 - kl_loss: 66.7731 - val_loss: 20.9910 - val_threeD_loss: 20.3232 - val_kl_loss: 66.7723
Epoch 57/100
2253555/2253555 - 604s - loss: 20.9783 - threeD_loss: 20.3105 - kl_loss: 66.7728 - val_loss: 20.9910 - val_threeD_loss: 20.3233 - val_kl_loss: 66.7722
Epoch 58/100

Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.
2253555/2253555 - 604s - loss: 20.9786 - threeD_loss: 20.3109 - kl_loss: 66.7730 - val_loss: 20.9913 - val_threeD_loss: 20.3236 - val_kl_loss: 66.7723
Epoch 59/100
2253555/2253555 - 604s - loss: 20.9786 - threeD_loss: 20.3109 - kl_loss: 66.7728 - val_loss: 20.9911 - val_threeD_loss: 20.3234 - val_kl_loss: 66.7721
Epoch 60/100

Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.
2253555/2253555 - 601s - loss: 20.9785 - threeD_loss: 20.3108 - kl_loss: 66.7731 - val_loss: 20.9910 - val_threeD_loss: 20.3233 - val_kl_loss: 66.7721
Epoch 61/100
2253555/2253555 - 610s - loss: 20.9784 - threeD_loss: 20.3107 - kl_loss: 66.7730 - val_loss: 20.9913 - val_threeD_loss: 20.3236 - val_kl_loss: 66.7722
Epoch 62/100

Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.
2253555/2253555 - 602s - loss: 20.9784 - threeD_loss: 20.3106 - kl_loss: 66.7727 - val_loss: 20.9914 - val_threeD_loss: 20.3236 - val_kl_loss: 66.7723
Epoch 00062: early stopping
saving model to models/run_101
